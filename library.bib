@inproceedings{LeFeuvre2016,
	abstract = {Interactive spatial navigation within a video is getting attention in the research community, especially in scenarios such as 360¬∞ video, video surveillance or region-of-interest viewing. With more content being delivered over the top, the complexity of spatial access to the video is increased by the need to perform bitrate adaptation to react to network variations. In this paper, we describe how spatial access can be performed in an adaptive HTTP streaming context, using tiling of the source content, MPEG-DASH and its SRD extensions. We describe a configurable implementation of these technologies, within the GPAC open-source player, allowing experimentations of different adaptation policies for tiled video content.},
	address = {New York, NY, USA},
	author = {{Le Feuvre}, Jean and Concolato, Cyril},
	booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
	doi = {10.1145/2910017.2910641},
	file = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le Feuvre, Concolato - 2016 - Tiled-based adaptive streaming using MPEG-DASH(3).pdf:pdf},
	isbn = {9781450342971},
	keywords = {HEVC,HTTP adaptive streaming,MPEG-DASH,Spatial description,Video tiling},
	month = {may},
	pages = {1--3},
	publisher = {ACM},
	title = {{Tiled-based adaptive streaming using MPEG-DASH}},
	url = {https://dl.acm.org/doi/10.1145/2910017.2910641},
	year = {2016}
}


@misc{ITU-T2018,
	author = {ITU-T},
	file = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ITU-T - 2018 - High Efficiency Video Coding. Recommendation ITU-T H.265.pdf:pdf},
	keywords = {Standard},
	mendeley-groups = {Padr{\~{o}}es e Recomenda{\c{c}}{\~{o}}es},
	mendeley-tags = {Standard},
	publisher = {ITU-T},
	title = {{High Efficiency Video Coding. Recommendation ITU-T H.265}},
	year = {2018}
}


@article{Sullivan2012a,
	abstract = {High Efficiency Video Coding (HEVC) is currently being prepared as the newest video coding standard of the ITU-T Video Coding Experts Group and the ISO/IEC Moving Picture Experts Group. The main goal of the HEVC standardization effort is to enable significantly improved compression performance relative to existing standards-in the range of 50% bit-rate reduction for equal perceptual video quality. This paper provides an overview of the technical features and characteristics of the HEVC standard.},
	author = {Sullivan, Gary J. and Ohm, Jens Rainer and Han, Woo Jin and Wiegand, Thomas},
	doi = {10.1109/TCSVT.2012.2221191},
	file = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sullivan et al. - 2012 - Overview of the high efficiency video coding (HEVC) standard.pdf:pdf},
	isbn = {1051-8215 VO - 22},
	issn = {10518215},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Advanced video coding (AVC),H.264,High Efficiency Video Coding (HEVC),Joint Collaborative Team on Video Coding (JCT-VC),MPEG-4,Moving Picture Experts Group (MPEG),Video Coding Experts Group (VCEG),standards,video compression},
	number = {12},
	pages = {1649--1668},
	title = {{Overview of the high efficiency video coding (HEVC) standard}},
	volume = {22},
	year = {2012}
}


@article{ITU-TRecommendationP.9192020,
	abstract = {SERIES P: TELEPHONE TRANSMISSION QUALITY, TELEPHONE INSTALLATIONS, LOCAL LINE NETWORKS - Audiovisual quality in multimedia services},
	author = {ITU-T},
	journal = {International Telecommunication Union, Geneva},
	pages = {1-42},
	title = {ITU-T Recommendation P. 919: Subjective test methodologies for 360¬∫ video on 
	head-mounted displays (Audiovisual Quality in Multimedia Services)},
	year = {2009},
}

@inproceedings{Zhou2017,
	abstract = {¬© 2017 ACM. 360 degree video is a new generation of video streaming technology that promises greater immersiveness than standard video streams. This level of immersiveness is similar to that produced by virtual reality devices - users can control the field of view using head movements rather than needing to manipulate external devices. Although 360 degree video could revolutionize streaming technology, large scale adoption is hindered by a number of factors. 360 degree video streams have larger bandwidth requirements, require faster responsiveness to user inputs, and users may be more sensitive to lower quality streams. In this paper, we review standard approaches toward 360 degree video encoding and compare these to a new, as yet unpublished, approach by Oculus which we refer to as the offset cubic projection. Compared to the standard cubic encoding, the offset cube encodes a distorted version of the spherical surface, devoting more information (i.e., pixels) to the view in a chosen direction. We estimate that the offset cube representation can produce better or similar visual quality while using less than 50% pixels under reasonable assumptions about user behavior, resulting in 5.6% to 16.4% average savings in video bitrate. During 360 degree video streaming, Oculus uses a combination of quality level adaptation and view orientation adaptation. We estimate that this combination of streaming adaptation in two dimensions can cause over 57% extra segments to be downloaded compared to an ideal downloading strategy, wasting 20% of the total downloading bandwidth.},
	author = {Chao Zhou and Zhenhua Li and Yao Liu},
	city = {New York, NY, USA},
	doi = {10.1145/3083187.3083190},
	isbn = {9781450350020},
	journal = {Proceedings of the 8th ACM on Multimedia Systems Conference},
	keywords = {-  Computing methodologies  ->  Virtual reality,-  Information systems  ->  Multimedia streaming,360 degree video streaming,o set cubic projection,visual quality},
	month = {6},
	pages = {27-37},
	publisher = {ACM},
	title = {A Measurement Study of Oculus 360 Degree Video Streaming},
	url = {https://dl.acm.org/doi/10.1145/3083187.3083190},
	year = {2017},
}

@inproceedings{Yi2019,
	abstract = {360¬∞ live video streaming is becoming increasingly popular. While providing viewers with enriched experience, 360¬∞ live video streaming is challenging to achieve since it requires a significantly higher bandwidth and a powerful computation infrastructure. A deeper understanding of this emerging system would benefit both viewers and system designers. Although prior works have extensively studied regular video streaming and 360¬∞ video on demand streaming, we for the first time investigate the performance of 360¬∞ live video streaming. We conduct a systematic measurement of YouTube's 360¬∞ live video streaming using various metrics in multiple practical settings. Our key findings suggest that viewers are advised not to live stream 4K 360¬∞ video, even when dynamic adaptive streaming over HTTP (DASH) is enabled. Instead, 1080p 360¬∞ live video can be played smoothly. However, the extremely large one-way video delay makes it only feasible for delay-tolerant broadcasting applications rather than real-time interactive applications. More importantly, we have concluded from our results that the primary design weakness of current systems lies in inefficient server processing, non-optimal rate adaptation, and conservative buffer management. Our research insight will help to build a clear understanding of today's 360¬∞ live video streaming and lay a foundation for future research on this emerging yet relatively unexplored area.},
	author = {Jun Yi and Shiqing Luo and Zhisheng Yan},
	city = {New York, NY, USA},
	doi = {10.1145/3304112.3325613},
	isbn = {9781450362986},
	journal = {Proceedings of the 29th ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
	keywords = {360¬∞,DASH,Live video streaming,Measurement study,Virtual reality},
	month = {6},
	pages = {49-54},
	publisher = {ACM},
	title = {A measurement study of YouTube 360¬∞ live video streaming},
	url = {https://dl.acm.org/doi/10.1145/3304112.3325613},
	year = {2019},
}


@inproceedings{Liu2017,
	author = {Liu, Xing and Xiao, Qingyang and Gopalakrishnan, Vijay and Han, Bo and Qian, Feng and Varvello, Matteo},
	title = {360¬∞ Innovations for Panoramic Video Streaming},
	year = {2017},
	isbn = {9781450355698},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3152434.3152443},
	doi = {10.1145/3152434.3152443},
	abstract = {360-degree videos are becoming increasingly popular on commercial platforms. In this position paper, we propose a holistic research agenda aiming at improving the performance, resource utilization efficiency, and users' quality of experience (QoE) for 360¬∞ video streaming on commodity mobile devices. Based on a Field-of-View (FoV) guided approach that fetches only portions of a scene that users will see, our proposed research includes the following: robust video rate adaptation with incremental chunk upgrading, big-data-assisted head movement prediction and rate adaptation, novel support for multipath streaming, and enhancements to live 360¬∞ video broadcast. We also show preliminary results demonstrating promising performance of our proof-of-concept 360¬∞ video streaming system on which our proposed research are being prototyped, integrated, and evaluated.},
	booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
	pages = {50‚Äì56},
	numpages = {7},
	location = {<conf-loc>, <city>Palo Alto</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
	series = {HotNets '17}
}

@ARTICLE{Alface2012,
	author={Alface, Patrice Rondao and Macq, Jean-Fran√ßois and Verzijp, Nico},
	journal={Bell Labs Technical Journal}, 
	title={Interactive omnidirectional video delivery: A bandwidth-effective approach}, 
	year={2012},
	volume={16},
	number={4},
	pages={135-147},
	keywords={},
	doi={10.1002/bltj.20538}}


@ARTICLE{Bovik2009,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}}



@InProceedings{Afzal2017,
  author    = {Afzal, Shahryar and Chen, Jiasi and Ramakrishnan, K. K.},
  title     = {{Characterization of 360-degree Videos}},
  booktitle = {Proceedings of the Workshop on Virtual Reality and Augmented Reality Network - VR/AR Network '17},
  year      = {2017},
  pages     = {1--6},
  address   = {New York, New York, USA},
  month     = {aug},
  publisher = {ACM Press},
  abstract  = {Online streaming of Virtual Reality and 360 ‚Ä¢ videos is rapidly grow-ing, as more and more major content providers and news outlets adopt the format to enrich the user experience. We characterize 360 ‚Ä¢ videos by examining several thousand YouTube videos across more than a dozen categories. 360 ‚Ä¢ videos, at trst sight, seem to pose a challenge for the network to stream because of their substan-tially higher bit rates and larger number of resolutions. However, a careful examination of video characteristics reveals that there are signiicant opportunities for reducing the actual bit rate delivered to client devices based on the user's seld of view. We study the bit rate and the motion in 360 ‚Ä¢ videos, and compare them against regular videos by investigating several important metrics. We end that 360 ‚Ä¢ videos are less variable in terms of bit rate, and have less motion than regular videos. Our expectation is that variability in the bit rates due to the motion of the camera in regular videos (or switching between cameras) is now translated to responsiveness requirements for end to end 360 ‚Ä¢ streaming architectures.},
  doi       = {10.1145/3097895.3097896},
  file      = {:C$\backslash$:/Users/henri/Desktop/Mendeley PDF/2017 - Afzal, Chen, Ramakrishnan - Characterization of 360-degree Videos.pdf:pdf},
  isbn      = {9781450350556},
  keywords  = {2017,360,360¬∞ videos,Measurements,Video delivery,acm reference format,characterization of,jiasi chen,k,measurements,ramakrishnan,shahryar afzal,video delivery,videos},
  url       = {http://dl.acm.org/citation.cfm?doid=3097895.3097896},
}

@TechReport{Cisco2020,
  author      = {Cisco},
  institution = {Cisco},
  title       = {{2020 Global Networking Trends Report}},
  year        = {2020},
  url         = {https://www.cisco.com/c/m/en_us/solutions/enterprise-networks/networking-report.html},
}

@InProceedings{Zare2016,
  author          = {Zare, Alireza and Aminlou, Alireza and Hannuksela, Miska M. and Gabbouj, Moncef},
  booktitle       = {Proceedings of the 2016 ACM on Multimedia Conference - MM '16},
  title           = {{HEVC-compliant Tile-based Streaming of Panoramic Video for Virtual Reality Applications}},
  year            = {2016},
  address         = {New York, New York, USA},
  month           = {oct},
  pages           = {601--605},
  publisher       = {ACM Press},
  abstract        = {Delivering wide-angle and high-resolution spherical panoramic video content entails a high streaming bitrate. This imposes challenges when panorama clips are consumed in virtual reality (VR) head-mounted displays (HMD). The reason is that the HMDs typically require high spatial and temporal fidelity contents and strict low-latency in order to guarantee the user's sense of presence while using them. In order to alleviate the problem, we propose to store two versions of the same video content at different resolutions, each divided into multiple tiles using the High Efficiency Video Coding (HEVC) standard. According to the user's present viewport, a set of tiles is transmitted in the highest captured resolution, while the remaining parts are transmitted from the low-resolution version of the same content. In order to enable selectively choosing different combinations, the tile sets are encoded to be independently decodable. We further study the trade- off in the choice of tiling scheme and its impact on compression and streaming bitrate performances. The results indicate streaming bitrate saving from 30% to 40%, depending on the selected tiling scheme, when compared to streaming the entire video content.},
  doi             = {10.1145/2964284.2967292},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zare et al. - 2016 - HEVC-compliant Tile-based Streaming of Panoramic Video for Virtual Reality Applications.pdf:pdf},
  isbn            = {9781450336031},
  keywords        = {HEVC,Head-mounted display (HMD),Panoramic video streaming,Tiles,Video coding,Virtual reality,hevc,in,multimedia and vision,panoramic video streaming,support of vision applications,tiles,video coding,video coding and streaming,virtual},
  mendeley-groups = {EI2020,360 video/Streaming},
  url             = {https://www.researchgate.net/publication/308820373_HEVC-compliant_Tile-based_Streaming_of_Panoramic_Video_for_Virtual_Reality_Applications http://dl.acm.org/citation.cfm?doid=2964284.2967292},
}

@InProceedings{Hosseini2017,
  author    = {Hosseini, Mohammad and Swaminathan, Viswanathan},
  booktitle = {Proceedings - 2016 IEEE International Symposium on Multimedia, ISM 2016},
  title     = {{Adaptive 360 VR video streaming based on MPEG-DASH SRD}},
  year      = {2017},
  month     = {jan},
  number    = {December},
  pages     = {407--408},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  abstract  = {We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system based on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR videos, and showcase a dynamic view-aware adaptation technique to tackle the high bandwidth demands of streaming 360 VR videos to wireless VR headsets. We spatially partition the underlying 3D mesh into multiple 3D sub-meshes, and construct an efficient 3D geometry mesh called hexaface sphere to optimally represent tiled 360 VR videos in the 3D space. We then spatially divide the 360 videos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe the spatial relationship of tiles in the 3D space, and prioritize the tiles in the Field of View (FoV) for view-aware adaptation. Our initial evaluation results show that we can save up to 72% of the required bandwidth on 360 VR video streaming with minor negative quality impacts compared to the baseline scenario when no adaptations is applied.},
  doi       = {10.1109/ISM.2016.44},
  file      = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosseini, Swaminathan - 2017 - Adaptive 360 VR video streaming based on MPEG-DASH SRD.pdf:pdf},
  isbn      = {9781509045709},
  url       = {https://ieeexplore.ieee.org/document/7823660},
}

@InProceedings{Qian2018,
  author          = {Qian, Feng and Han, Bo and Xiao, Qingyang and Gopalakrishnan, Vijay},
  booktitle       = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking - MobiCom '18},
  title           = {{Flare: Practical Viewport-Adaptive 360-Degree Video Streaming for Mobile Devices}},
  year            = {2018},
  address         = {New York, New York, USA},
  number          = {Ml},
  pages           = {99--114},
  publisher       = {ACM Press},
  abstract        = {Flare is a practical system for streaming 360¬∞ videos on commodity mobile devices. It takes a viewport-adaptive approach , which fetches only portions of a panoramic scene that cover what a viewer is about to perceive. We conduct an IRB-approved user study where we collect head movement traces from 130 diverse users to gain insights on how to design the viewport prediction mechanism for Flare. We then develop novel online algorithms that determine which spatial portions to fetch and their corresponding qualities. We also innovate other components in the streaming pipeline such as decoding and server-side transmission. Through extensive evaluations (‚àº400 hours' playback on WiFi and ‚àº100 hours over LTE), we show that Flare significantly improves the QoE in real-world settings. Compared to non-viewport-adaptive approaches, Flare yields up to 18√ó quality level improvement on WiFi, and achieves high bandwidth reduction (up to 35%) and video quality enhancement (up to 4.9√ó) on LTE.},
  doi             = {10.1145/3241539.3241565},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2018 - Flare Practical Viewport-Adaptive 360-Degree Video Streaming for Mobile Devices.pdf:pdf},
  isbn            = {9781450359030},
  mendeley-groups = {EI2020,360 video/Streaming,SVR},
  url             = {https://doi.org/10.1145/3241539.3241565 http://dl.acm.org/citation.cfm?doid=3241539.3241565},
}

@InProceedings{Huang2014,
  author          = {Huang, Te-Yuan and Johari, Ramesh and McKeown, Nick and Trunnell, Matthew and Watson, Mark},
  booktitle       = {Proceedings of the 2014 ACM conference on SIGCOMM - SIGCOMM '14},
  title           = {{A Buffer-Based Approach to Rate Adaptation: Evidence from a Large Video Streaming Service}},
  year            = {2014},
  address         = {New York, New York, USA},
  pages           = {187--198},
  publisher       = {ACM Press},
  abstract        = {To provide a better streaming experience, video clients today select their video rates by observing and estimating the available capacity. Recent work has shown that capacity estimation is fraught with difficulties because of complex interactions between the ABR control loop, HTTP server performance and TCP congestion control. Estimation-based rate selection algorithms can lead to unnecessary rebuffering events and suboptimal video quality. This paper argues that we should do away with estimating network capacity, and instead directly observe and control the playback buffer--which is the state variable we are most interested in controlling. We present a class of "buffer-based" rate selection algorithms that reduce the rebuffering rate while allowing us to control the delivered video quality. We implemented our algorithms inside the Netflix video client and ran a series of experiments spanning millions of Netflix users around the world. Our results show that by doing away with estimating network capacity and instead focusing on buffer occupancy, we can reduce rebuffer rates by 20% while holding video rate constant.},
  archiveprefix   = {arXiv},
  arxivid         = {1401.2209},
  doi             = {10.1145/2619239.2626296},
  eprint          = {1401.2209},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2014 - A buffer-based approach to rate adaptation.pdf:pdf},
  isbn            = {9781450328364},
  issn            = {19435819},
  keywords        = {http-based video streaming,video rate adaptation al-},
  mendeley-groups = {EI2020,Video Streaming (1),Anothers Network papers},
  url             = {http://dl.acm.org/citation.cfm?doid=2619239.2626296},
}

@InProceedings{Yadav2017,
  author          = {Yadav, Praveen Kumar and Shafiei, Arash and Ooi, Wei Tsang},
  booktitle       = {Proceedings of the 2017 ACM on Multimedia Conference - MM '17},
  title           = {{QUETRA: A Queuing Theory Approach to DASH Rate Adaptation}},
  year            = {2017},
  address         = {New York, New York, USA},
  pages           = {1130--1138},
  publisher       = {ACM Press},
  abstract        = {DASH, or Dynamic Adaptive Streaming over HTTP, relies on a rate adaptation component to decide on which representation to download for each video segment. A plethora of rate adaptation al- gorithms has been proposed in recent years. The decisions of which bitrate to download made by these algorithms largely depend on several factors: estimated network throughput, buffer occupancy, and buffer capacity. Yet, these algorithms are not informed by a fun- damental relationship between these factors and the chosen bitrate, and as a result, we found that they do not perform consistently in all scenarios, and require parameter tuning to work well under different buffer capacity. In this paper, we model a DASH client as an M / D / 1 / K queue, which allows us to calculate the expected buffer occupancy given a bitrate choice, network throughput, and buffer capacity. Using this model, we propose QUETRA, a simple rate adaptation algorithm. We evaluated QUETRA under a diverse set of scenarios and found that, despite its simplicity, it leads to better quality of experience (7% - 140%) than existing algorithms.},
  doi             = {10.1145/3123266.3123390},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yadav, Shafiei, Ooi - 2017 - QUETRA A Queuing Theory Approach to DASH Rate Adaptation.pdf:pdf},
  isbn            = {9781450349062},
  keywords        = {DASH,HTTP streaming,dash,http streaming,queuing model,rate adaptation},
  mendeley-groups = {Video Streaming (1)},
  url             = {http://dl.acm.org/citation.cfm?doid=3123266.3123390},
}

@InProceedings{Graf2017,
  author          = {Graf, Mario and Timmerer, Christian and Mueller, Christopher},
  booktitle       = {Proceedings of the 8th ACM on Multimedia Systems Conference - MMSys'17},
  title           = {{Towards Bandwidth Efficient Adaptive Streaming of Omnidirectional Video over HTTP}},
  year            = {2017},
  address         = {New York, New York, USA},
  pages           = {261--271},
  publisher       = {ACM Press},
  abstract        = {Real-time entertainment services such as streaming audio- visual content deployed over the open, unmanaged Internet account now for more than 70% during peak periods. More and more such bandwidth hungry applications and services are proposed like immersive media services such as virtual reality and, speci cally omnidirectional/360-degree videos. The adaptive streaming of omnidirectional video over HTTP imposes an important challenge on today's video delivery infrastructures which calls for dedicated, thoroughly designed techniques for content generation, delivery, and consumption. This paper describes the usage of tiles|as speci ed within modern video codecs such HEVC/H.265 and VP9 | enabling bandwidth ecient adaptive streaming of omnidirectional video over HTTP and we de ne various streaming strategies. Therefore, the parameters and characteristics of a dataset for omnidirectional video are proposed and exemplary instanti- ated to evaluate various aspects of such an ecosystem, namely bitrate overhead, bandwidth requirements, and quality as- pects in terms of viewport PSNR. The results indicate bitrate savings from 40% (in a realistic scenario with recorded head movements from real users) up to 65% (in an ideal scenario with a centered/ xed viewport) and serve as a baseline and guidelines for advanced techniques including the outline of a research roadmap for the near future.},
  annote          = {3x2, 5x3, 6x4, 8x5},
  doi             = {10.1145/3083187.3084016},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graf, Timmerer, Mueller - 2017 - Towards Bandwidth Efficient Adaptive Streaming of Omnidirectional Video over HTTP.pdf:pdf},
  isbn            = {9781450350020},
  keywords        = {265 tiles,360-degree video,acm reference format,h,hevc,im-,mersive media,mpeg-dash,omnidirectional video,tiled streaming},
  mendeley-groups = {360 video/Streaming},
  url             = {http://dl.acm.org/citation.cfm?doid=3083187.3084016},
}

@InProceedings{Yin2015,
  author          = {Yin, Xiaoqi and Jindal, Abhishek and Sekar, Vyas and Sinopoli, Bruno},
  booktitle       = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication - SIGCOMM '15},
  title           = {{A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP}},
  year            = {2015},
  address         = {New York, New York, USA},
  pages           = {325--338},
  publisher       = {ACM Press},
  abstract        = {User-perceived quality-of-experience (QoE) is critical in Internet video applications as it impacts revenues for content providers and delivery systems. Given that there is little support in the network for optimizing such measures, bottlenecks could occur anywhere in the delivery system. Consequently, a robust bitrate adaptation algorithm in client-side players is critical to ensure good user experience. Previous studies have shown key limitations of state-of-art commercial solutions and proposed a range of heuristic fixes. Despite the emergence of several proposals, there is still a distinct lack of consensus on: (1) How best to design this client-side bitrate adaptation logic (e.g., use rate estimates vs. buffer occupancy); (2) How well specific classes of approaches will perform under diverse operating regimes (e.g., high throughput variability); or (3) How do they actually balance different QoE objectives (e.g., startup delay vs. rebuffering). To this end, this paper makes three key technical contributions. First, to bring some rigor to this space, we develop a principled control-theoretic model to reason about a broad spectrum of strategies. Second, we propose a novel model predictive control algorithm that can optimally combine throughput and buffer occupancy information to outperform traditional approaches. Third, we present a practical implementation in a reference video player to validate our approach using realistic trace-driven emulations.},
  doi             = {10.1145/2785956.2787486},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yin et al. - 2015 - A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP.pdf:pdf},
  isbn            = {9781450335423},
  keywords        = {Bitrate adaptation,DASH,Internet video,Model predictive control},
  mendeley-groups = {EI2020},
  url             = {http://dl.acm.org/citation.cfm?doid=2785956.2787486},
}

@Article{Xiao2018,
  author          = {Xiao, Mengbai and Zhou, Chao and Swaminathan, Viswanathan and Liu, Yao and Chen, Songqing},
  journal         = {Proceedings - IEEE INFOCOM},
  title           = {{BAS-360¬∞: Exploring Spatial and Temporal Adaptability in 360-degree Videos over HTTP/2}},
  year            = {2018},
  issn            = {0743166X},
  pages           = {953--961},
  volume          = {2018-April},
  abstract        = {Today, 360-degree video streaming has become a popular Internet service with the rise of affordable virtual reality (VR) technologies. However, streaming 360-degree videos suffers from the prohibitive bandwidth demand. Existing bandwidth-efficient solutions mainly focus on exploiting the inherent spatial adaptability of 360-degree videos, delivering only video content (spatially-cut tiles) in the viewer's region of interest (ROI) with higher quality. Temporal adaptability, which has been widely leveraged in HTTP streaming, has not been well exploited to select proper quality for video segments according to the bandwidth variations. When these two dimensions of adaptability are jointly considered, bitrate selection for the tiles become more complicated and challenging. The importance of a tile with a spatial coordination played at a specific time should be quantified so that we can determine how to allocate bandwidth for improving the viewer's quality of experience. Furthermore, viewer's head orientation prediction is highly variable, which makes the determination of important tiles highly dynamic. In addition, network fluctuations are very common on the Internet. To overcome these challenges, we propose Bi-Adaptive Streaming for 360-degree videos (BAS-360¬∞). In BAS-360¬∞, both spatial and temporal adaptabilities are explored in the bitrate selection for different tiles. The objective is to minimize the bandwidth waste by allocating bandwidth to more important tiles (the tiles that are more likely to be watched). To tackle the high variability of visual region prediction and the unpredictable network fluctuations, we employ two features provided by HTT P /2: stream termination and stream priority, to efficiently organize tile delivery. Evaluation results show that BAS-360¬∞ outperforms naive tile-based 360-degree video streaming strategies when network fluctuations or errors in viewport predictions occur. {\textcopyright} 2018 IEEE.},
  doi             = {10.1109/INFOCOM.2018.8486390},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao et al. - 2018 - BAS-360¬∞ Exploring Spatial and Temporal Adaptability in 360-degree Videos over HTTP2.pdf:pdf},
  isbn            = {9781538641286},
  keywords        = {360-degree video streaming,HTTP streaming,HTTP/2},
  mendeley-groups = {EI2020,360 video/Streaming},
}

@InProceedings{Spiteri2016,
  author          = {Spiteri, Kevin and Urgaonkar, Rahul and Sitaraman, Ramesh K},
  booktitle       = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
  title           = {{BOLA: Near-optimal bitrate adaptation for online videos}},
  year            = {2016},
  month           = {apr},
  pages           = {1--9},
  publisher       = {IEEE},
  volume          = {2016-July},
  abstract        = {Modern video players employ complex algorithms to adapt the bitrate of the video that is shown to the user. Bitrate adaptation requires a tradeoff between reducing the probability that the video freezes and enhancing the quality of the video shown to the user. A bitrate that is too high leads to frequent video freezes (i.e., rebuffering), while a bitrate that is too low leads to poor video quality. Video providers segment the video into short chunks and encode each chunk at multiple bitrates. The video player adaptively chooses the bitrate of each chunk that is downloaded, possibly choosing different bitrates for successive chunks. While bitrate adaptation holds the key to a good quality of experience for the user, current video players use ad-hoc algorithms that are poorly understood. We formulate bitrate adaptation as a utility maximization problem and devise an online control algorithm called BOLA that uses Lyapunov optimization techniques to minimize rebuffering and maximize video quality.We prove that BOLA achieves a time-average utility that is within an additive term O(1/V) of the optimal value, for a control parameter V related to the video buffer size. Further, unlike prior work, our algorithm does not require any prediction of available network bandwidth. We empirically validate our algorithm in a simulated network environment using an extensive collection of network traces. We show that our algorithm achieves near-optimal utility and in many cases significantly higher utility than current state-of-the-art algorithms. Our work has immediate impact on real-world video players and for the evolving DASH standard for video transmission.},
  archiveprefix   = {arXiv},
  arxivid         = {1601.06748v2},
  doi             = {10.1109/INFOCOM.2016.7524428},
  eprint          = {1601.06748v2},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Spiteri, Urgaonkar, Sitaraman - 2016 - BOLA Near-optimal bitrate adaptation for online videos.pdf:pdf},
  isbn            = {978-1-4673-9953-1},
  issn            = {0743166X},
  keywords        = {Adaptive Bitrate Streaming,Internet Video,Lyapunov Optimization,Optimal Control,Video Quality},
  mendeley-groups = {EI2020,Anothers Network papers},
  url             = {http://ieeexplore.ieee.org/document/7524428/},
}
@ARTICLE{wspsnr2017,
  author={Sun, Yule and Lu, Ang and Yu, Lu},
  journal={IEEE Signal Processing Letters}, 
  title={Weighted-to-Spherically-Uniform Quality Evaluation for Omnidirectional Video}, 
  year={2017},
  volume={24},
  number={9},
  pages={1408-1412},
  keywords={Distortion measurement;Extraterrestrial measurements;Distortion;Two dimensional displays;Weight measurement;Measurement uncertainty;Objective quality evaluation;omnidirectional video;projection format},
  doi={10.1109/LSP.2017.2720693}}


@Article{Concolato2017,
  author          = {Concolato, Cyril and {Le Feuvre}, Jean and Denoual, Franck and Maze, Frederic and Nassor, Eric and Ouedraogo, Nael and Taquet, Jonathan},
  journal         = {IEEE Transactions on Circuits and Systems for Video Technology},
  title           = {{Adaptive Streaming of HEVC Tiled Videos Using MPEG-DASH}},
  year            = {2018},
  issn            = {1051-8215},
  month           = {aug},
  number          = {8},
  pages           = {1981--1992},
  volume          = {28},
  abstract        = {www.sena.edu.co Presentaci{\'{o}}n El presente documento fue desarrollado por el Observatorio Laboral y Ocupacional Colombia-no (OLO) y tiene como fin describir las principales caracter{\'{i}}sticas del sector dise{\~{n}}o, confec-ci{\'{o}}n y moda en cuanto a su estructura productiva y ocupacional, los programas ofertados por el SENA, y los certificados de normas de competencias laborales relacionadas con el sector. La primera secci{\'{o}}n del documento presenta una caracterizaci{\'{o}}n general del sector, y la segunda secci{\'{o}}n se encarga de presentar el contexto laboral y ocupacional del sector en Colombia y su relaci{\'{o}}n con el SENA en t{\'{e}}rminos de programas de formaci{\'{o}}n y certificaci{\'{o}}n en normas de competencia.},
  doi             = {10.1109/TCSVT.2017.2688491},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Concolato et al. - 2018 - Adaptive Streaming of HEVC Tiled Videos Using MPEG-DASH.pdf:pdf},
  keywords        = {Adaptive streaming,DASH,HEVC,HTTP streaming,MPEG,video tiling},
  mendeley-groups = {360 video/Streaming,SVR},
  url             = {http://ieeexplore.ieee.org/document/7888522/ https://ieeexplore.ieee.org/document/7888522/},
}

@InProceedings{Nasrabadi2019,
  author          = {Nasrabadi, Afshin Taghavi and Samiei, Aliehsan and Mahzari, Anahita and Farias, Mylene C. Q. and Carvalho, Marcelo M. and McMahan, Ryan P. and Prakash, Ravi},
  booktitle       = {Proceedings of the 10th ACM on Multimedia Systems Conference - MMSys'19},
  title           = {{A Taxonomy and Dataset for 360 deg Videos}},
  year            = {2019},
  month           = {may},
  pages           = {2--7},
  abstract        = {In this paper, we propose a taxonomy for 360{\deg} videos that categorizes videos based on moving objects and camera motion. We gathered and produced 28 videos based on the taxonomy, and recorded viewport traces from 60 participants watching the videos. In addition to the viewport traces, we provide the viewers' feedback on their experience watching the videos, and we also analyze viewport patterns on each category.},
  archiveprefix   = {arXiv},
  arxivid         = {1905.03823},
  doi             = {10.1145/3304109.3325812},
  eprint          = {1905.03823},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nasrabadi et al. - 2019 - A Taxonomy and Dataset for 360 deg Videos.pdf:pdf},
  isbn            = {9781450362979},
  keywords        = {360,dataset,video,viewport,virtual reality},
  mendeley-groups = {360 video/Head Movement Dataset},
  url             = {http://arxiv.org/abs/1905.03823%0Ahttp://dx.doi.org/10.1145/3304109.3325812 http://arxiv.org/abs/1905.03823 http://dx.doi.org/10.1145/3304109.3325812},
}

@InProceedings{Qian2016,
  author          = {Qian, Feng and Ji, Lusheng and Han, Bo and Gopalakrishnan, Vijay},
  booktitle       = {Proceedings of the 5th Workshop on All Things Cellular Operations, Applications and Challenges - ATC '16},
  title           = {{Optimizing 360 video delivery over cellular networks}},
  year            = {2016},
  address         = {New York, New York, USA},
  pages           = {1--6},
  publisher       = {ACM Press},
  abstract        = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing ‚Äúflavors‚Äù. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.},
  doi             = {10.1145/2980055.2980056},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2016 - Optimizing 360 video delivery over cellular networks.pdf:pdf},
  isbn            = {9781450342490},
  keywords        = {360-degree video,cel-,head movement prediction,virtual reality},
  mendeley-groups = {360 video/Streaming,SVR},
  url             = {http://dl.acm.org/citation.cfm?doid=2980055.2980056},
}

@InProceedings{Tran2017,
  author          = {Tran, Huyen T.T. T and Ngoc, Nam Pham and Pham, Cuong T. and Jung, Yong Ju and Thang, Truong Cong},
  booktitle       = {2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)},
  title           = {{A subjective study on QoE of 360 video for VR communication}},
  year            = {2017},
  month           = {oct},
  pages           = {1--6},
  publisher       = {IEEE},
  volume          = {2017-Janua},
  abstract        = {{\textcopyright} 2017 IEEE. Currently, more and more 360-degree videos (or 360 videos for short) are being provided via the Internet. This kind of videos can render a virtual reality (VR) environment via a headmounted display (HMD). However, understanding the quality of experience (QoE) of 360 videos is a big challenge because user experience in VR is a very complex phenomenon. In this paper, the QoE of 360 videos is considered in terms of four aspects, namely perceptual quality, presence, acceptability, and cybersickness. Subjective tests are designed to investigate the influences of important factors including encoding parameters, content characteristics, and device types on QoE aspects. In addition, a comparison of perceptual quality and acceptability between VR and non-VR rendering modes, which refer to watching 360 videos with and without using an HMD respectively, is also made in this study. To the best of our knowledge, this is the first study that covers these four QoE aspects and a large number of influence factors.},
  doi             = {10.1109/MMSP.2017.8122249},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tran et al. - 2017 - A subjective study on QoE of 360 video for VR communication.pdf:pdf},
  isbn            = {978-1-5090-3649-3},
  keywords        = {360 video,Quality of experience,Subjective test,Virtual reality,quality of experience,virtual reality},
  mendeley-groups = {360 video/Quality},
  url             = {http://ieeexplore.ieee.org/document/8122249/},
}

@Article{Xu2020,
  author        = {Xu, Mai and Li, Chen and Zhang, Shanyi and Callet, Patrick Le},
  journal       = {IEEE Journal of Selected Topics in Signal Processing},
  title         = {{State-of-the-Art in 360¬∞ Video/Image Processing: Perception, Assessment and Compression}},
  year          = {2020},
  issn          = {1932-4553},
  month         = {jan},
  number        = {1},
  pages         = {5--26},
  volume        = {14},
  abstract      = {Nowadays, 360{\deg} video/image has been increasingly popular and drawn great attention. The spherical viewing range of 360{\deg} video/image accounts for huge data, which pose the challenges to 360{\deg} video/image processing in solving the bottleneck of storage, transmission, etc. Accordingly, the recent years have witnessed the explosive emergence of works on 360{\deg} video/image processing. In this paper, we review the state-of-the-art works on 360{\deg} video/image processing from the aspects of perception, assessment and compression. First, this paper reviews both datasets and visual attention modelling approaches for 360{\deg} video/image. Second, we survey the related works on both subjective and objective visual quality assessment (VQA) of 360{\deg} video/image. Third, we overview the compression approaches for 360{\deg} video/image, which either utilize the spherical characteristics or visual attention models. Finally, we summarize this overview paper and outlook the future research trends on 360{\deg} video/image processing.},
  archiveprefix = {arXiv},
  arxivid       = {1905.00161},
  doi           = {10.1109/JSTSP.2020.2966864},
  eprint        = {1905.00161},
  file          = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2020 - State-of-the-Art in 360¬∞ VideoImage Processing Perception, Assessment and Compression.pdf:pdf},
  publisher     = {Institute of Electrical and Electronics Engineers (IEEE)},
  url           = {https://ieeexplore.ieee.org/document/8960364/},
}

@Article{T.T.Tran2019,
  author          = {{T. T. Tran}, Huyen and Ngoc, Nam P. and Pham, Cuong T. and Jung, Yong Ju and Thang, Truong Cong},
  journal         = {Applied Sciences},
  title           = {{A Subjective Study on User Perception Aspects in Virtual Reality}},
  year            = {2019},
  issn            = {2076-3417},
  month           = {aug},
  number          = {16},
  pages           = {3384},
  volume          = {9},
  abstract        = {Three hundred and sixty degree video is becoming more and more popular on the Internet. By using a Head-Mounted Display, 360-degree video can render a Virtual Reality (VR) environment. However, it is still a big challenge to understand Quality of Experience (QoE) of 360-degree video since user experience during watching 360-degree video is a very complex phenomenon. In this paper, we aim to investigate four QoE aspects of 360-degree video, namely, perceptual quality, presence, cybersickness, and acceptability. In addition, four key QoE-affecting factors of encoding parameters, content motion, rendering device, and rendering mode are considered in our study. To the best of our knowledge, this is the first work that covers a large number of factors and QoE aspects of 360-degree video. In this study, a subjective experiment is conducted using 60 video versions generated from three original 360-degree videos. Based on statistical analysis of the obtained results, various findings on the impacts of the factors on the QoE aspects are provided. In particular, regarding the impacts of encoding parameters, it is found that the difference of QoE is negligible between video versions encoded at 4 K and 2.5 K resolutions. Also, it is suggested that 360-degree video should not be encoded at HD resolution or lower when watching in VR mode using Head Mounted Display. In addition, the bitrate for good QoE varies widely across different video contents. With respect to the content motion factor, its impact is statistically significant on the perceptual quality, presence, and cybersickness. In a comparison of two rendering device sets used in this study, there is no statistically significant difference found for the acceptability and cybersickness. However, the differences of the perceptual quality and presence are indicated to be statistically significant. Regarding the rendering mode, a comparison between VR and non-VR modes is also conducted. Although the non-VR mode always achieves higher perceptual quality scores and higher acceptability rates, more than a half of the viewers prefer the VR mode to the non-VR mode when watching versions encoded at the resolutions of fHD or higher. By contrast, the non-VR mode is preferred at the HD resolution.},
  doi             = {10.3390/app9163384},
  file            = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/T. T. Tran et al. - 2019 - A Subjective Study on User Perception Aspects in Virtual Reality.pdf:pdf},
  mendeley-groups = {360 video/Subjetive Analysis},
  url             = {https://www.mdpi.com/2076-3417/9/16/3384},
}

@Article{Nguyen2020,
  author   = {Nguyen, Duc V. and Tran, Huyen T. T. and Thang, Truong Cong},
  journal  = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  title    = {{An Evaluation of Tile Selection Methods for Viewport-Adaptive Streaming of 360-Degree Video}},
  year     = {2020},
  issn     = {1551-6857},
  month    = {apr},
  number   = {1},
  pages    = {1--24},
  volume   = {16},
  abstract = {360-degree video has become increasingly popular nowadays. For effective transmission of bandwidth-intensive 360-degree video over networks, viewport-adaptive streaming has been introduced. In this article, we evaluate, for the first time, ten existing methods to understand the effectiveness of tile-based viewport adaptive streaming of 360-degree video. Experimental results show that tile-based methods can improve the average V-PSNR by up to 4.3 dB compared to a non-tiled method under low delay settings. Here, the V-PSNR is computed as the peak signal-to-noise ratio of the adapted viewport compared to the corresponding origin viewport. Also, different methods show different tradeoffs between average viewport quality and viewport quality variations. Especially, the performances of most tile-based methods decrease quickly as the segment duration and/or buffer size increase for the content with no main focus. Even, under long delay settings like HTTP Adaptive Streaming, it is found that the simple non-tiled method appears to be the best one. For the content with a strong viewing focus, it is found that the tile-based methods are less influenced by the segment duration and the buffer size. In addition, a comparison of the performances of the tile selection methods using two popular viewport estimation methods is conducted. It is interesting that there is only little difference found in performances of tile selection methods. The findings of this study are useful for service providers to make decisions on deployment of streaming solutions.},
  doi      = {10.1145/3373359},
  file     = {:C\:/Users/Henrique/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Tran, Thang - 2020 - An Evaluation of Tile Selection Methods for Viewport-Adaptive Streaming of 360-Degree Video.pdf:pdf},
  keywords = {360-degree Video,Viewport Adaptive Streaming,Virtual Reality},
  url      = {https://dl.acm.org/doi/10.1145/3373359},
}

@Comment{jabref-meta: databaseType:bibtex;}
